# main.py - Complete Python Crawl4AI + Supabase Integration

"""
Main orchestration script for the Python-based crawl4ai + Supabase solution.

This script demonstrates how to use the complete Python stack to replace
Edge Functions entirely, providing 90% cost reduction through schema-based extraction.
"""

import asyncio
import os
from dotenv import load_dotenv
from supabase_handler import SupabaseHandler, crawl_with_schema

# Load environment variables
load_dotenv()


async def main():
    """Main demonstration of the Python crawl4ai + Supabase integration"""

    print("ğŸš€ Python Crawl4AI + Supabase Integration Demo")
    print("=" * 60)

    # Initialize Supabase handler
    print("ğŸ“Š Initializing Supabase connection...")
    handler = SupabaseHandler()

    # Check system health
    print("\nğŸ” Checking system health...")
    health = handler.check_health()
    print(f"Database Status: {health['status']}")
    if health["status"] == "healthy":
        print(f"âœ… Connected to: {health['postgres_version']}")
        print(
            f"âœ… Vector Extension: {'Enabled' if health['vector_extension'] else 'Disabled'}"
        )

    # Show current stats
    print("\nğŸ“ˆ Current Database Statistics:")
    stats = handler.get_stats()
    for key, value in stats.items():
        print(f"  {key.replace('_', ' ').title()}: {value}")

    # Example: Create a sample extraction schema
    print("\nğŸ”§ Setting up Alberta Regulatory extraction schema...")
    alberta_schema = {
        "name": "Alberta Regulation Records",
        "baseSelector": "body",
        "fields": [
            {
                "name": "title",
                "selector": "h1, title",
                "type": "text",
                "required": True,
            },
            {"name": "content", "selector": "p, div.content, main", "type": "text"},
            {
                "name": "links",
                "selector": "a[href]",
                "attribute": "href",
                "type": "list",
            },
            {
                "name": "metadata",
                "selector": "meta[name='description']",
                "attribute": "content",
                "type": "text",
            },
        ],
        "transform": "flatten",
    }

    schema_id = handler.save_extraction_schema(
        name="alberta_regulatory",
        description="Extraction schema for Alberta regulatory websites",
        base_url="https://www.alberta.ca/",
        patterns=alberta_schema,
    )
    print(f"âœ… Schema saved with ID: {schema_id}")

    # Example: Perform schema-based crawl
    print("\nğŸ¯ Performing schema-based crawl (90% cost reduction)...")
    test_url = "https://www.alberta.ca/mineral-ownership"

    try:
        result_id = await crawl_with_schema(handler, test_url, "alberta_regulatory")
        if result_id:
            print(f"âœ… Crawl completed - Result ID: {result_id}")

            # Retrieve and display result
            result = handler.get_crawl_result(test_url)
            if result:
                print(f"\nğŸ“„ Crawl Result Summary:")
                print(f"  Title: {result['title']}")
                print(f"  Content Length: {result['content_length']} characters")
                print(f"  Extraction Quality: {result['extraction_quality']}")
                print(f"  Schema Match Score: {result['schema_match_score']}")
        else:
            print("âŒ Crawl failed")
    except Exception as e:
        print(f"âŒ Crawl error: {e}")

    # Show updated stats
    print("\nğŸ“Š Updated Database Statistics:")
    updated_stats = handler.get_stats()
    for key, value in updated_stats.items():
        print(f"  {key.replace('_', ' ').title()}: {value}")

    print("\nâœ… Demo complete!")
    print("\nğŸ’¡ Key Benefits of Python Solution:")
    print("  â€¢ 90% cost reduction through schema-based extraction")
    print("  â€¢ No Edge Function compatibility issues")
    print("  â€¢ Full control over crawling pipeline")
    print("  â€¢ Direct PostgreSQL performance")
    print("  â€¢ Production-proven architecture patterns")


if __name__ == "__main__":
    asyncio.run(main())
